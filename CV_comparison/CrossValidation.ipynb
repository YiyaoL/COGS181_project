{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c12db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3975d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest1 = pd.read_csv(\"vh_vv_data_new_20.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest2 = pd.read_csv(\"vh_vv_data_new_20_100.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest3 = pd.read_csv(\"vh_vv_data_new_100_200.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest4 = pd.read_csv(\"vh_vv_data_new_200_225.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest5 = pd.read_csv(\"vh_vv_data_new_225_300.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest6 = pd.read_csv(\"vh_vv_data_new_300_350.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest7 = pd.read_csv(\"vh_vv_data_new_350_400.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest8 = pd.read_csv(\"vh_vv_data_new_400_450.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest9 = pd.read_csv(\"vh_vv_data_new_450_500.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest10 = pd.read_csv(\"vh_vv_data_new_500_557.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest = pd.concat([dftest1,dftest2,dftest3,dftest4, dftest5,dftest6, dftest7, dftest8, dftest9,dftest10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d24aeb",
   "metadata": {},
   "source": [
    "### Prepare data (modified to select 550 random points for CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f1f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_from_df_pad(string):\n",
    "    \"\"\"\n",
    "    takes in each cell as a string and replaces it with an array of float value. \n",
    "    Also pads the sequence to length 26\n",
    "    \"\"\"\n",
    "    splitted = string.split(\" \")\n",
    "    result = []\n",
    "    for i in splitted:\n",
    "        number = float(i.replace(\",\",\"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "        result.append(number)\n",
    "    if len(result) !=26:\n",
    "        missing = 26-len(result)\n",
    "        comp = [0] * missing\n",
    "        result = result + comp\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa93267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in those dataframes again before this\n",
    "dftest[\"vv_list\"] = dftest.vv_list.apply(clean_from_df_pad)\n",
    "dftest[\"vh_list\"] = dftest.vh_list.apply(clean_from_df_pad)\n",
    "dftest['vv/vh_list'] = dftest['vv/vh_list'].apply(clean_from_df_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42d8109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a climate index called rvi\n",
    "def calculate_rvi(vv, vh):\n",
    "    \"\"\"\n",
    "    takes in a list of vvs and a list of vhs, return a list of their rvis\n",
    "    dop = vv/(vv+vh)\n",
    "    m = 1-dop\n",
    "    m = np.sqrt(m)\n",
    "    power_func = 4*vh/(vv+vh)\n",
    "    rvi = m*power_func\n",
    "    \"\"\"\n",
    "    if len(vv)!= len(vh):\n",
    "        return None\n",
    "    else:\n",
    "        rvi_lst = []\n",
    "        for i in range(len(vv)):\n",
    "            rvi = np.sqrt(1-vv[i]/(vv[i]+vh[i])) * 4*vh[i]/(vv[i]+vh[i])\n",
    "            rvi_lst.append(rvi)\n",
    "    return rvi_lst\n",
    "\n",
    "calculate_rvi_func = np.vectorize(calculate_rvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377b7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest[\"rvi\"] = calculate_rvi(list(dftest.vv_list), list(dftest.vh_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1af2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length is equal\n",
    "def df_to_arr_equal_len (df):\n",
    "    \"\"\"\n",
    "    takes in a df, which will be data, and transforms it into a 3-d array with its original size\n",
    "    \"\"\"\n",
    "    # loop through every row in df\n",
    "    res = []\n",
    "    for i in range(df.shape[0]):\n",
    "        row = list(df.iloc[i,:]) # each row of df is turned into a list of three elements, where each element is an array\n",
    "        new_row = [] # one day, one location, three variables\n",
    "        for var in row:\n",
    "            new_row.append(var)\n",
    "        res.append(new_row)\n",
    "    return np.asarray(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919d3898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2504"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = df_to_arr_equal_len(dftest)\n",
    "np.isnan(arr).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0684c568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace null rvi's with 0\n",
    "arr = np.nan_to_num(arr, nan=0)\n",
    "np.isnan(arr).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7059c536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557, 4, 26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ed123f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# get gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "768dbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(arr, batch_size, num_epochs, print_freq, model_type):\n",
    "    \"\"\"\n",
    "    takes in a model and data of size [557,26,4]. batch_size is 50, 10 , or None\n",
    "    Applies 11-fold cross validation and returns criterion for each fold\n",
    "    check that model has batch_size 10 or 50\n",
    "    first randomly select 550 samples from the whole sample set\n",
    "    divide these 550 samples into 11 parts\n",
    "    for i in range(0,11):\n",
    "    testing set would be the ith part, which has length 50\n",
    "    train the model on the rest parts\n",
    "    do the test\n",
    "    and print the results\n",
    "    \"\"\"\n",
    "    # prepare dataset\n",
    "    cv_indices = random.sample(range(0,557), 550) # already in random order\n",
    "    cv_input = arr\n",
    "    cv_target = np.asarray(pd.read_csv(\"Crop_Yield_Data_challenge_2.csv\")[\"Rice Yield (kg/ha)\"])\n",
    "    \n",
    "    # get it into 11 parts\n",
    "    cv_indices_grouped = np.asarray(cv_indices).reshape((11,50)) # each row is a group\n",
    "    \n",
    "    #training_losses_lst = []\n",
    "    testing_metrices_lst = []\n",
    "    for i in range(0,11):\n",
    "        print(\"Testing Fold: \", i)\n",
    "        # create a new model\n",
    "        if model_type == \"RNN\":\n",
    "            model = MyRNN(input_size=4, hidden_size=64, num_layers=5, output_size=1, \n",
    "                          batch_size=50, nonlinearity=\"tanh\", dropout = 0.1)\n",
    "        if model_type ==\"GRU\":\n",
    "            model = GRU2(input_size=4, hidden_size=100, output_size = 1, num_layers=10, \n",
    "                 batch_first=True, batch_size =10)\n",
    "        if model_type ==\"LSTM\":\n",
    "            model = MyLSTM(input_size=4, hidden_size=300, output_size =1, num_layers=8)\n",
    "            \n",
    "        model.to(device)\n",
    "        criterion = nn.L1Loss() # training criterion\n",
    "        \n",
    "        # partition data and select group\n",
    "        testing_input = cv_input[cv_indices_grouped[i]] # use the ith group as testing\n",
    "        testing_target = cv_target[cv_indices_grouped[i]]\n",
    "        training_input = cv_input[[j for j in cv_indices if j not in cv_indices_grouped[i]]] # use the rest 10 groups for training\n",
    "        training_target = cv_target[[j for j in cv_indices if j not in cv_indices_grouped[i]]]\n",
    "        \n",
    "        # change them into datasets and create dataloaders\n",
    "        testing_input = torch.from_numpy(testing_input).float().view(50,26,4)\n",
    "        testing_target = torch.from_numpy(testing_target).float().view(50)\n",
    "        testing_dataloader = DataLoader(MyDataset(testing_input, testing_target), batch_size = batch_size, shuffle=True)\n",
    "        \n",
    "        training_input = torch.from_numpy(training_input).float().view(500,26,4)\n",
    "        training_target = torch.from_numpy(training_target).float().view(500)\n",
    "        training_dataloader = DataLoader(MyDataset(training_input, training_target), batch_size = batch_size, shuffle=True)\n",
    "        \n",
    "        # train the model on the training set and test its performance on the testing set\n",
    "        if batch_size != None:\n",
    "            model, training_losses = train_model_batch(training_dataloader, model, num_epochs, print_freq, model_type)\n",
    "            test_results = test_model_batch(testing_dataloader, model) # a list of 4 metrices\n",
    "            testing_metrices_lst.append(test_results)\n",
    "        \n",
    "        else:\n",
    "            model, training_losses = train_model_no_batch(training_input, training_target, model, num_epochs, print_freq)\n",
    "            test_results = test_model_no_batch(testing_input, testing_target, model) # a list of 4 metrices\n",
    "            testing_metrices_lst.append(test_results)\n",
    "    print(\"Finished CV\")\n",
    "    return (testing_metrices_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "068b5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_batch(training_dataloader, model, num_epochs, print_freq, model_type):\n",
    "    losses= []\n",
    "    if model_type ==\"GRU\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.03, momentum = 0.9)\n",
    "    if model_type ==\"RNN\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.008)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(training_dataloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(running_loss/ len(training_dataloader))\n",
    "        if epoch% print_freq ==1:\n",
    "            print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss / len(training_dataloader)))\n",
    "    print(\"finished training\")\n",
    "    return (model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e3cae9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_no_batch(training_input, training_target, model, num_epochs, print_freq):\n",
    "    \"\"\"\n",
    "    takes in the training input and target separtely, and trains the model. Used for LSTMbest\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    training_input, training_target = training_input.to(device), training_target.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.09, momentum=0.9)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output_seq = model(training_input)\n",
    "        loss = criterion(output_seq.squeeze(), training_target)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm = 1.0, norm_type=2, error_if_nonfinite=False)\n",
    "        optimizer.step()\n",
    "        if epoch % print_freq == 1:\n",
    "            print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, loss.item()))\n",
    "            #print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    print(\"Finished training\")\n",
    "    return (model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc870fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_batch(testing_dataloader, model):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset, return four metrices, all averaged across batches\n",
    "    takes in a dataloader\n",
    "    \"\"\"\n",
    "    total_explained_variance = 0.0\n",
    "    total_MSE= 0.0\n",
    "    total_MAE = 0.0\n",
    "    total_r2 = 0.0\n",
    "    for batch in testing_dataloader:\n",
    "        inputs, targets = batch[0].to(device), batch[1].cpu().numpy()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.data.cpu().numpy()\n",
    "        total_explained_variance += explained_variance_score(y_true=targets, y_pred=outputs)\n",
    "        total_MSE += mean_squared_error(y_true=targets, y_pred=outputs)\n",
    "        total_MAE += mean_absolute_error(y_true=targets,y_pred=outputs)\n",
    "        total_r2 += r2_score(y_true=targets,y_pred=outputs)\n",
    "    a = len(testing_dataloader)\n",
    "    #print(\"Explained varaince, MSE, MAE, R2: \\n\")\n",
    "    return ([total_explained_variance/a, total_MSE/a, total_MAE/a, total_r2/a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2bce4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_no_batch(testing_input, testing_target, model):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset, return four metrices, all averaged across batches\n",
    "    takes in the inputs and targets separately\n",
    "    \"\"\"\n",
    "    inputs = testing_input.to(device)\n",
    "    targets = testing_target.cpu().numpy()\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.data.cpu().numpy()\n",
    "    total_explained_variance = explained_variance_score(y_true=targets, y_pred=outputs)\n",
    "    total_MSE = mean_squared_error(y_true=targets, y_pred=outputs)\n",
    "    total_MAE = mean_absolute_error(y_true=targets,y_pred=outputs)\n",
    "    total_r2 = r2_score(y_true=targets,y_pred=outputs)\n",
    "    #print(\"Explained varaince, MSE, MAE, R2: \\n\")\n",
    "    return([total_explained_variance, total_MSE, total_MAE, total_r2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98e02f",
   "metadata": {},
   "source": [
    "---\n",
    "### Create models\n",
    "- Best RNN: RNN model 4. Hidden_size = 64, num_layers = 5, batch_size = 50, optimizer = Adam(learning rate = 0.008), dropout = 0.1. Copied code from notebook Imrpove_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07ca57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline RNN, with batch norm \n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size, nonlinearity, \n",
    "                 dropout, bidirectional=False):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.randn(self.num_layers, self.batch_size, self.hidden_size).requires_grad_().to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.bn(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "933adc07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Fold:  0\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6620.6317\n",
      "Epoch [102/400], Loss: 682.0594\n",
      "Epoch [202/400], Loss: 682.1132\n",
      "Epoch [302/400], Loss: 682.2592\n",
      "finished training\n",
      "Testing Fold:  1\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6651.5147\n",
      "Epoch [102/400], Loss: 693.0118\n",
      "Epoch [202/400], Loss: 692.4015\n",
      "Epoch [302/400], Loss: 692.6214\n",
      "finished training\n",
      "Testing Fold:  2\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6657.0917\n",
      "Epoch [102/400], Loss: 679.1424\n",
      "Epoch [202/400], Loss: 678.8175\n",
      "Epoch [302/400], Loss: 678.5776\n",
      "finished training\n",
      "Testing Fold:  3\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6637.5587\n",
      "Epoch [102/400], Loss: 693.6181\n",
      "Epoch [202/400], Loss: 693.6710\n",
      "Epoch [302/400], Loss: 693.7761\n",
      "finished training\n",
      "Testing Fold:  4\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6616.4670\n",
      "Epoch [102/400], Loss: 687.0939\n",
      "Epoch [202/400], Loss: 687.1891\n",
      "Epoch [302/400], Loss: 687.2466\n",
      "finished training\n",
      "Testing Fold:  5\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6608.5480\n",
      "Epoch [102/400], Loss: 680.5791\n",
      "Epoch [202/400], Loss: 680.7361\n",
      "Epoch [302/400], Loss: 680.6804\n",
      "finished training\n",
      "Testing Fold:  6\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6609.7842\n",
      "Epoch [102/400], Loss: 687.4275\n",
      "Epoch [202/400], Loss: 687.3676\n",
      "Epoch [302/400], Loss: 687.9321\n",
      "finished training\n",
      "Testing Fold:  7\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6628.3811\n",
      "Epoch [102/400], Loss: 667.1812\n",
      "Epoch [202/400], Loss: 667.0005\n",
      "Epoch [302/400], Loss: 667.1457\n",
      "finished training\n",
      "Testing Fold:  8\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6623.8340\n",
      "Epoch [102/400], Loss: 680.4740\n",
      "Epoch [202/400], Loss: 680.5245\n",
      "Epoch [302/400], Loss: 680.4502\n",
      "finished training\n",
      "Testing Fold:  9\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6639.6406\n",
      "Epoch [102/400], Loss: 681.3891\n",
      "Epoch [202/400], Loss: 681.4419\n",
      "Epoch [302/400], Loss: 681.6943\n",
      "finished training\n",
      "Testing Fold:  10\n",
      "MyRNN(\n",
      "  (rnn): RNN(4, 64, num_layers=5, batch_first=True)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [2/400], Loss: 6628.5646\n",
      "Epoch [102/400], Loss: 680.1695\n",
      "Epoch [202/400], Loss: 680.3143\n",
      "Epoch [302/400], Loss: 680.3950\n",
      "finished training\n",
      "Finished CV\n"
     ]
    }
   ],
   "source": [
    "RNN_cv_results = cross_validation(arr, batch_size=50, num_epochs=400, print_freq=100, model_type=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebafbdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN CV results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explained variance</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.913543e-04</td>\n",
       "      <td>648062.06250</td>\n",
       "      <td>691.936951</td>\n",
       "      <td>-0.078780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.179312e-04</td>\n",
       "      <td>506082.53125</td>\n",
       "      <td>607.801270</td>\n",
       "      <td>-0.075194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>717951.43750</td>\n",
       "      <td>749.620605</td>\n",
       "      <td>-0.099635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.384186e-07</td>\n",
       "      <td>516508.62500</td>\n",
       "      <td>577.372498</td>\n",
       "      <td>-0.004436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>603303.81250</td>\n",
       "      <td>640.863037</td>\n",
       "      <td>-0.116493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7.748604e-06</td>\n",
       "      <td>689362.56250</td>\n",
       "      <td>708.264893</td>\n",
       "      <td>-0.199020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.429983e-04</td>\n",
       "      <td>579992.50000</td>\n",
       "      <td>643.106506</td>\n",
       "      <td>-0.229817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.517504e-05</td>\n",
       "      <td>956042.62500</td>\n",
       "      <td>844.587341</td>\n",
       "      <td>-0.017233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-5.966425e-04</td>\n",
       "      <td>690654.87500</td>\n",
       "      <td>709.087036</td>\n",
       "      <td>-0.051448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>638227.81250</td>\n",
       "      <td>699.486389</td>\n",
       "      <td>-0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>720343.18750</td>\n",
       "      <td>711.108826</td>\n",
       "      <td>-0.028112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Explained variance           MSE         MAE        R2\n",
       "0         6.913543e-04  648062.06250  691.936951 -0.078780\n",
       "1         3.179312e-04  506082.53125  607.801270 -0.075194\n",
       "2         0.000000e+00  717951.43750  749.620605 -0.099635\n",
       "3        -2.384186e-07  516508.62500  577.372498 -0.004436\n",
       "4         0.000000e+00  603303.81250  640.863037 -0.116493\n",
       "5        -7.748604e-06  689362.56250  708.264893 -0.199020\n",
       "6         5.429983e-04  579992.50000  643.106506 -0.229817\n",
       "7         8.517504e-05  956042.62500  844.587341 -0.017233\n",
       "8        -5.966425e-04  690654.87500  709.087036 -0.051448\n",
       "9         0.000000e+00  638227.81250  699.486389 -0.000966\n",
       "10        0.000000e+00  720343.18750  711.108826 -0.028112"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RNN CV results\")\n",
    "RNNdf = pd.DataFrame(np.asarray(RNN_cv_results))\n",
    "RNNdf.columns=[\"Explained variance\", \"MSE\", \"MAE\", \"R2\"]\n",
    "RNNdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cecda3",
   "metadata": {},
   "source": [
    "---\n",
    "### Best GRU\n",
    "GRU model 5. Hidden_size = 100, num_layers = 10, batch_size = 10, optimizer = SGD(learning rate = 0.01, momentum = 0.9), dropout = 0.1\n",
    "- copied from notebook Improve_GRU\n",
    "- **NOTE: CHANGED LEARNING RATE TO 0.03 TO GET FASTER RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e3f0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, \n",
    "                 batch_first, batch_size):\n",
    "        super(GRU2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, \n",
    "                  batch_first=True, dropout=0.1, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.bn = nn.BatchNorm1d(num_features=self.hidden_size, affine = False)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.randn(self.gru.num_layers, batch_size, self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.bn(out[:,-1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb13fc6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Fold:  0\n",
      "Epoch [2/600], Loss: 6595.8692\n",
      "Epoch [102/600], Loss: 5095.8206\n",
      "Epoch [202/600], Loss: 3595.5766\n",
      "Epoch [302/600], Loss: 2095.6776\n",
      "Epoch [402/600], Loss: 920.1450\n",
      "Epoch [502/600], Loss: 712.0618\n",
      "finished training\n",
      "Testing Fold:  1\n",
      "Epoch [2/600], Loss: 6610.9317\n",
      "Epoch [102/600], Loss: 5110.8831\n",
      "Epoch [202/600], Loss: 3610.6392\n",
      "Epoch [302/600], Loss: 2110.7402\n",
      "Epoch [402/600], Loss: 917.1657\n",
      "Epoch [502/600], Loss: 694.4841\n",
      "finished training\n",
      "Testing Fold:  2\n",
      "Epoch [2/600], Loss: 6610.4758\n",
      "Epoch [102/600], Loss: 5110.4272\n",
      "Epoch [202/600], Loss: 3610.1832\n",
      "Epoch [302/600], Loss: 2110.2845\n",
      "Epoch [402/600], Loss: 906.9373\n",
      "Epoch [502/600], Loss: 673.5804\n",
      "finished training\n",
      "Testing Fold:  3\n",
      "Epoch [2/600], Loss: 6627.6890\n",
      "Epoch [102/600], Loss: 5127.6405\n",
      "Epoch [202/600], Loss: 3627.3964\n",
      "Epoch [302/600], Loss: 2127.4977\n",
      "Epoch [402/600], Loss: 931.4793\n",
      "Epoch [502/600], Loss: 694.6684\n",
      "finished training\n",
      "Testing Fold:  4\n",
      "Epoch [2/600], Loss: 6611.6348\n",
      "Epoch [102/600], Loss: 5111.5863\n",
      "Epoch [202/600], Loss: 3611.3421\n",
      "Epoch [302/600], Loss: 2111.4431\n",
      "Epoch [402/600], Loss: 910.9525\n",
      "Epoch [502/600], Loss: 678.9217\n",
      "finished training\n",
      "Testing Fold:  5\n",
      "Epoch [2/600], Loss: 6586.3618\n",
      "Epoch [102/600], Loss: 5086.3134\n",
      "Epoch [202/600], Loss: 3586.0694\n",
      "Epoch [302/600], Loss: 2086.1703\n",
      "Epoch [402/600], Loss: 900.6469\n",
      "Epoch [502/600], Loss: 692.5489\n",
      "finished training\n",
      "Testing Fold:  6\n",
      "Epoch [2/600], Loss: 6609.7303\n",
      "Epoch [102/600], Loss: 5109.6818\n",
      "Epoch [202/600], Loss: 3609.4378\n",
      "Epoch [302/600], Loss: 2109.5391\n",
      "Epoch [402/600], Loss: 912.9366\n",
      "Epoch [502/600], Loss: 687.9793\n",
      "finished training\n",
      "Testing Fold:  7\n",
      "Epoch [2/600], Loss: 6629.0037\n",
      "Epoch [102/600], Loss: 5128.9551\n",
      "Epoch [202/600], Loss: 3628.7111\n",
      "Epoch [302/600], Loss: 2128.8124\n",
      "Epoch [402/600], Loss: 928.7926\n",
      "Epoch [502/600], Loss: 691.2932\n",
      "finished training\n",
      "Testing Fold:  8\n",
      "Epoch [2/600], Loss: 6602.2487\n",
      "Epoch [102/600], Loss: 5102.2003\n",
      "Epoch [202/600], Loss: 3601.9564\n",
      "Epoch [302/600], Loss: 2102.0576\n",
      "Epoch [402/600], Loss: 911.0510\n",
      "Epoch [502/600], Loss: 679.3278\n",
      "finished training\n",
      "Testing Fold:  9\n",
      "Epoch [2/600], Loss: 6615.9223\n",
      "Epoch [102/600], Loss: 5115.8738\n",
      "Epoch [202/600], Loss: 3615.6299\n",
      "Epoch [302/600], Loss: 2115.7308\n",
      "Epoch [402/600], Loss: 922.4234\n",
      "Epoch [502/600], Loss: 696.4516\n",
      "finished training\n",
      "Testing Fold:  10\n",
      "Epoch [2/600], Loss: 6620.8370\n",
      "Epoch [102/600], Loss: 5120.7884\n",
      "Epoch [202/600], Loss: 3620.5443\n",
      "Epoch [302/600], Loss: 2120.6455\n",
      "Epoch [402/600], Loss: 928.7176\n",
      "Epoch [502/600], Loss: 695.4947\n",
      "finished training\n",
      "Finished CV\n"
     ]
    }
   ],
   "source": [
    "GRU_cv_results = cross_validation(arr, batch_size=10, num_epochs=600, print_freq=100, model_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "124c4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU CV results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explained variance</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000041</td>\n",
       "      <td>468164.46250</td>\n",
       "      <td>561.378479</td>\n",
       "      <td>-0.266563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000190</td>\n",
       "      <td>618182.21250</td>\n",
       "      <td>643.558917</td>\n",
       "      <td>-0.119006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>866154.76250</td>\n",
       "      <td>819.188965</td>\n",
       "      <td>-0.173293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>469954.76250</td>\n",
       "      <td>615.200354</td>\n",
       "      <td>-0.304278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000163</td>\n",
       "      <td>822290.27500</td>\n",
       "      <td>767.333105</td>\n",
       "      <td>-0.066486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000100</td>\n",
       "      <td>755496.08750</td>\n",
       "      <td>734.756726</td>\n",
       "      <td>-0.410618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>692739.28750</td>\n",
       "      <td>690.345813</td>\n",
       "      <td>-0.157411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>574712.09375</td>\n",
       "      <td>651.396497</td>\n",
       "      <td>-0.014103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.000044</td>\n",
       "      <td>775363.02500</td>\n",
       "      <td>765.190955</td>\n",
       "      <td>-0.198012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.000144</td>\n",
       "      <td>543813.21875</td>\n",
       "      <td>621.243823</td>\n",
       "      <td>-0.048067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000171</td>\n",
       "      <td>601505.55000</td>\n",
       "      <td>629.144922</td>\n",
       "      <td>-0.011754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Explained variance           MSE         MAE        R2\n",
       "0            -0.000041  468164.46250  561.378479 -0.266563\n",
       "1             0.000190  618182.21250  643.558917 -0.119006\n",
       "2            -0.000012  866154.76250  819.188965 -0.173293\n",
       "3             0.000085  469954.76250  615.200354 -0.304278\n",
       "4            -0.000163  822290.27500  767.333105 -0.066486\n",
       "5            -0.000100  755496.08750  734.756726 -0.410618\n",
       "6             0.000068  692739.28750  690.345813 -0.157411\n",
       "7            -0.000005  574712.09375  651.396497 -0.014103\n",
       "8            -0.000044  775363.02500  765.190955 -0.198012\n",
       "9            -0.000144  543813.21875  621.243823 -0.048067\n",
       "10           -0.000171  601505.55000  629.144922 -0.011754"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GRU CV results\")\n",
    "GRUdf = pd.DataFrame(np.asarray(GRU_cv_results))\n",
    "GRUdf.columns=[\"Explained variance\", \"MSE\", \"MAE\", \"R2\"]\n",
    "GRUdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49214431",
   "metadata": {},
   "source": [
    "---\n",
    "### Best LSTM\n",
    "LSTM model 2. Hidden_size = 300, num_layers = 8, no batch norm layer, optimizer = SGD(learning rate = 0.03, momentum = 0.9), dropout = 0\n",
    "- copied from notebook Improve_LSTM\n",
    "- **NOTE: CHANGED LEARNING RATE FROM 0.03 TO 0.09 TO GET FASTER RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2032e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        h0, c0 = h0.to(device), c0.to(device)\n",
    "        lstm_out, _ = self.lstm(input, (h0, c0))\n",
    "        output = self.linear(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf4b9951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Fold:  0\n",
      "Epoch [2/600], Loss: 6639.6626\n",
      "Epoch [102/600], Loss: 5418.0366\n",
      "Epoch [202/600], Loss: 3856.7141\n",
      "Epoch [302/600], Loss: 2295.2795\n",
      "Epoch [402/600], Loss: 870.6371\n",
      "Epoch [502/600], Loss: 662.4599\n",
      "Finished training\n",
      "Testing Fold:  1\n",
      "Epoch [2/600], Loss: 6609.5532\n",
      "Epoch [102/600], Loss: 5376.8452\n",
      "Epoch [202/600], Loss: 3815.4868\n",
      "Epoch [302/600], Loss: 2254.0520\n",
      "Epoch [402/600], Loss: 852.3378\n",
      "Epoch [502/600], Loss: 680.7924\n",
      "Finished training\n",
      "Testing Fold:  2\n",
      "Epoch [2/600], Loss: 6635.9175\n",
      "Epoch [102/600], Loss: 5411.2812\n",
      "Epoch [202/600], Loss: 3849.9290\n",
      "Epoch [302/600], Loss: 2288.4946\n",
      "Epoch [402/600], Loss: 872.9791\n",
      "Epoch [502/600], Loss: 678.1746\n",
      "Finished training\n",
      "Testing Fold:  3\n",
      "Epoch [2/600], Loss: 6636.3481\n",
      "Epoch [102/600], Loss: 5414.5923\n",
      "Epoch [202/600], Loss: 3853.2546\n",
      "Epoch [302/600], Loss: 2291.8191\n",
      "Epoch [402/600], Loss: 869.6761\n",
      "Epoch [502/600], Loss: 667.5012\n",
      "Finished training\n",
      "Testing Fold:  4\n",
      "Epoch [2/600], Loss: 6611.7305\n",
      "Epoch [102/600], Loss: 5379.2334\n",
      "Epoch [202/600], Loss: 3817.8713\n",
      "Epoch [302/600], Loss: 2256.4368\n",
      "Epoch [402/600], Loss: 858.8306\n",
      "Epoch [502/600], Loss: 683.9034\n",
      "Finished training\n",
      "Testing Fold:  5\n",
      "Epoch [2/600], Loss: 6636.8174\n",
      "Epoch [102/600], Loss: 5410.8081\n",
      "Epoch [202/600], Loss: 3849.4553\n",
      "Epoch [302/600], Loss: 2288.0210\n",
      "Epoch [402/600], Loss: 868.7369\n",
      "Epoch [502/600], Loss: 669.1701\n",
      "Finished training\n",
      "Testing Fold:  6\n",
      "Epoch [2/600], Loss: 6635.7720\n",
      "Epoch [102/600], Loss: 5404.6030\n",
      "Epoch [202/600], Loss: 3843.2429\n",
      "Epoch [302/600], Loss: 2281.8093\n",
      "Epoch [402/600], Loss: 872.0157\n",
      "Epoch [502/600], Loss: 685.0362\n",
      "Finished training\n",
      "Testing Fold:  7\n",
      "Epoch [2/600], Loss: 6617.3291\n",
      "Epoch [102/600], Loss: 5387.0503\n",
      "Epoch [202/600], Loss: 3825.6921\n",
      "Epoch [302/600], Loss: 2264.2583\n",
      "Epoch [402/600], Loss: 858.6424\n",
      "Epoch [502/600], Loss: 677.3502\n",
      "Finished training\n",
      "Testing Fold:  8\n",
      "Epoch [2/600], Loss: 6626.2505\n",
      "Epoch [102/600], Loss: 5404.2900\n",
      "Epoch [202/600], Loss: 3843.0593\n",
      "Epoch [302/600], Loss: 2281.6233\n",
      "Epoch [402/600], Loss: 864.8068\n",
      "Epoch [502/600], Loss: 669.3956\n",
      "Finished training\n",
      "Testing Fold:  9\n",
      "Epoch [2/600], Loss: 6632.5723\n",
      "Epoch [102/600], Loss: 5409.0435\n",
      "Epoch [202/600], Loss: 3847.6938\n",
      "Epoch [302/600], Loss: 2286.2605\n",
      "Epoch [402/600], Loss: 879.1013\n",
      "Epoch [502/600], Loss: 685.9332\n",
      "Finished training\n",
      "Testing Fold:  10\n",
      "Epoch [2/600], Loss: 6634.9399\n",
      "Epoch [102/600], Loss: 5419.5176\n",
      "Epoch [202/600], Loss: 3858.1697\n",
      "Epoch [302/600], Loss: 2296.7358\n",
      "Epoch [402/600], Loss: 879.4519\n",
      "Epoch [502/600], Loss: 681.7435\n",
      "Finished training\n",
      "Finished CV\n"
     ]
    }
   ],
   "source": [
    "LSTM_cv_results = cross_validation(arr, batch_size=None, num_epochs=600, print_freq=100, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfa27557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM CV results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explained variance</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>817050.06250</td>\n",
       "      <td>816.748108</td>\n",
       "      <td>-0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>588380.25000</td>\n",
       "      <td>633.319641</td>\n",
       "      <td>-0.211024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>624038.87500</td>\n",
       "      <td>660.234558</td>\n",
       "      <td>-0.004812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>755863.37500</td>\n",
       "      <td>767.542480</td>\n",
       "      <td>-0.003228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>550588.37500</td>\n",
       "      <td>602.386475</td>\n",
       "      <td>-0.195227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>763971.93750</td>\n",
       "      <td>749.635010</td>\n",
       "      <td>-0.002836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>534230.37500</td>\n",
       "      <td>591.382996</td>\n",
       "      <td>-0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>641230.93750</td>\n",
       "      <td>667.585083</td>\n",
       "      <td>-0.103095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>745144.75000</td>\n",
       "      <td>746.799988</td>\n",
       "      <td>-0.034261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.192093e-07</td>\n",
       "      <td>513905.71875</td>\n",
       "      <td>583.262268</td>\n",
       "      <td>-0.016261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>558760.93750</td>\n",
       "      <td>623.235779</td>\n",
       "      <td>-0.008368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Explained variance           MSE         MAE        R2\n",
       "0         0.000000e+00  817050.06250  816.748108 -0.000375\n",
       "1         0.000000e+00  588380.25000  633.319641 -0.211024\n",
       "2         0.000000e+00  624038.87500  660.234558 -0.004812\n",
       "3         0.000000e+00  755863.37500  767.542480 -0.003228\n",
       "4         0.000000e+00  550588.37500  602.386475 -0.195227\n",
       "5         0.000000e+00  763971.93750  749.635010 -0.002836\n",
       "6         1.192093e-07  534230.37500  591.382996 -0.006100\n",
       "7         0.000000e+00  641230.93750  667.585083 -0.103095\n",
       "8         0.000000e+00  745144.75000  746.799988 -0.034261\n",
       "9        -1.192093e-07  513905.71875  583.262268 -0.016261\n",
       "10        0.000000e+00  558760.93750  623.235779 -0.008368"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LSTM CV results\")\n",
    "LSTMdf = pd.DataFrame(np.asarray(LSTM_cv_results))\n",
    "LSTMdf.columns=[\"Explained variance\", \"MSE\", \"MAE\", \"R2\"]\n",
    "LSTMdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb9611",
   "metadata": {},
   "source": [
    "______\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b24701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b71ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
