{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c12db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3975d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest1 = pd.read_csv(\"vh_vv_data_new_20.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest2 = pd.read_csv(\"vh_vv_data_new_20_100.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest3 = pd.read_csv(\"vh_vv_data_new_100_200.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest4 = pd.read_csv(\"vh_vv_data_new_200_225.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest5 = pd.read_csv(\"vh_vv_data_new_225_300.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest6 = pd.read_csv(\"vh_vv_data_new_300_350.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest7 = pd.read_csv(\"vh_vv_data_new_350_400.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest8 = pd.read_csv(\"vh_vv_data_new_400_450.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest9 = pd.read_csv(\"vh_vv_data_new_450_500.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest10 = pd.read_csv(\"vh_vv_data_new_500_557.csv\").drop(\"Unnamed: 0\", axis =1)\n",
    "dftest = pd.concat([dftest1,dftest2,dftest3,dftest4, dftest5,dftest6, dftest7, dftest8, dftest9,dftest10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501e809",
   "metadata": {},
   "source": [
    "### Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae279933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_from_df(string):\n",
    "    \"\"\"\n",
    "    takes in each cell as a string and replaces it with an array of float value\n",
    "    \"\"\"\n",
    "    splitted = string.split(\" \")\n",
    "    result = []\n",
    "    for i in splitted:\n",
    "        number = float(i.replace(\",\",\"\").replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "        result.append(number)\n",
    "    return np.asarray(result)\n",
    "\n",
    "dftest[\"vv_list\"] = dftest.vv_list.apply(clean_from_df)\n",
    "dftest[\"vh_list\"] = dftest.vh_list.apply(clean_from_df)\n",
    "dftest['vv/vh_list'] = dftest['vv/vh_list'].apply(clean_from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8706ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run those unimportant cells at the bottom first\n",
    "res = generate_stastical_features(dftest)\n",
    "res = pd.DataFrame(res ,columns = ['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
    "                          'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
    "                          'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh', 'permutation_entropy_vv_by_vh'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c1aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a climate index called rvi\n",
    "def calculate_rvi(vv, vh):\n",
    "    \"\"\"\n",
    "    takes in a vv and a vh, return their rvi\n",
    "    dop = vv/(vv+vh)\n",
    "    m = 1-dop\n",
    "    m = np.sqrt(m)\n",
    "    power_func = 4*vh/(vv+vh)\n",
    "    rvi = m*power_func\n",
    "    \"\"\"\n",
    "    rvi = np.sqrt(1-vv/(vv+vh)) * 4*vh/(vv+vh)\n",
    "    return rvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5460cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"rvi\"] = calculate_rvi(res['mean_vv'], res['mean_vh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e9ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine y variables\n",
    "y = pd.read_csv(\"Crop_Yield_Data_challenge_2.csv\")\n",
    "df = pd.concat([y, res], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc01ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data for extratree model\n",
    "X = df[['min_vv', 'max_vv', 'range_vv',\n",
    "       'mean_vv', 'correlation_vv', 'permutation_entropy_vv', 'min_vh',\n",
    "       'max_vh', 'range_vh', 'mean_vh', 'correlation_vh',\n",
    "       'permutation_entropy_vh', 'min_vv_by_vh', 'max_vv_by_vh',\n",
    "       'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh',\n",
    "       'permutation_entropy_vv_by_vh', 'rvi']]\n",
    "X = X.fillna((X.mean())).values\n",
    "#X = normalize(X, norm='l2')\n",
    "y = df ['Rice Yield (kg/ha)'].values\n",
    "# Choose any random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "540757be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insample R2 Score: 0.09\n",
      "Insample MAE : 633.52\n",
      "Outsample R2 Score: -0.06\n",
      "Outsample MAE : 753.00\n"
     ]
    }
   ],
   "source": [
    "regressor = ExtraTreesRegressor(bootstrap=True, ccp_alpha=0.001, criterion='mae',\n",
    "                    max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
    "                    max_samples=None, min_impurity_decrease=0.0, min_samples_leaf=20,\n",
    "                    min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
    "                    n_estimators=200, n_jobs=-1, oob_score=False,\n",
    "                    random_state=123, verbose=0, warm_start=False)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# training score\n",
    "insample_predictions = regressor.predict(X_train)\n",
    "print(\"Insample R2 Score: {0:.2f}\".format(r2_score(y_train,insample_predictions)))\n",
    "print(\"Insample MAE : {0:.2f}\".format(mean_absolute_error(y_train,insample_predictions)))\n",
    "\n",
    "\n",
    "# testing score\n",
    "outsample_predictions = regressor.predict(X_test)\n",
    "print(\"Outsample R2 Score: {0:.2f}\".format(r2_score(y_test,outsample_predictions)))\n",
    "print(\"Outsample MAE : {0:.2f}\".format(mean_absolute_error(y_test,outsample_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54d6c4",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "#### *Helper functions - calculate a single feature based on a time sequence*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61711d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_distribution(data, dx=3, dy=1, taux=1, tauy=1, return_missing=False, tie_precision=None):\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "     : tuple\n",
    "       Tuple containing two arrays, one with the ordinal patterns occurring in data \n",
    "       and another with their corresponding probabilities.\n",
    "       \n",
    "    Attributes\n",
    "    ---------\n",
    "    data : array \n",
    "           Array object in the format :math:`[x_{1}, x_{2}, x_{3}, \\\\ldots ,x_{n}]`\n",
    "           or  :math:`[[x_{11}, x_{12}, x_{13}, \\\\ldots, x_{1m}],\n",
    "           \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, \\\\ldots, x_{nm}]]`.\n",
    "    dx : int\n",
    "         Embedding dimension (horizontal axis) (default: 3).\n",
    "    dy : int\n",
    "         Embedding dimension (vertical axis); it must be 1 for time series \n",
    "         (default: 1).\n",
    "    taux : int\n",
    "           Embedding delay (horizontal axis) (default: 1).\n",
    "    tauy : int\n",
    "           Embedding delay (vertical axis) (default: 1).\n",
    "    return_missing: boolean\n",
    "                    If `True`, it returns ordinal patterns not appearing in the \n",
    "                    symbolic sequence obtained from **data** are shown. If `False`,\n",
    "                    these missing patterns (permutations) are omitted \n",
    "                    (default: `False`).\n",
    "    tie_precision : int\n",
    "                    If not `None`, **data** is rounded with `tie_precision`\n",
    "                    number of decimals (default: `None`).\n",
    "   \n",
    "    '''\n",
    "    def setdiff(a, b):\n",
    "        '''\n",
    "        Returns\n",
    "        -------\n",
    "        : array\n",
    "            An array containing the elements in `a` that are not contained in `b`.\n",
    "            \n",
    "        Parameters\n",
    "        ----------    \n",
    "        a : tuples, lists or arrays\n",
    "            Array in the format :math:`[[x_{21}, x_{22}, x_{23}, \\\\ldots, x_{2m}], \n",
    "            \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, ..., x_{nm}]]`.\n",
    "        b : tuples, lists or arrays\n",
    "            Array in the format :math:`[[x_{21}, x_{22}, x_{23}, \\\\ldots, x_{2m}], \n",
    "            \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, ..., x_{nm}]]`.\n",
    "        '''\n",
    "\n",
    "        a = np.asarray(a).astype('int64')\n",
    "        b = np.asarray(b).astype('int64')\n",
    "\n",
    "        _, ncols = a.shape\n",
    "\n",
    "        dtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
    "            'formats':ncols * [a.dtype]}\n",
    "\n",
    "        C = np.setdiff1d(a.view(dtype), b.view(dtype))\n",
    "        C = C.view(a.dtype).reshape(-1, ncols)\n",
    "\n",
    "        return(C)\n",
    "\n",
    "    try:\n",
    "        ny, nx = np.shape(data)\n",
    "        data   = np.array(data)\n",
    "    except:\n",
    "        nx     = np.shape(data)[0]\n",
    "        ny     = 1\n",
    "        data   = np.array([data])\n",
    "\n",
    "    if tie_precision is not None:\n",
    "        data = np.round(data, tie_precision)\n",
    "\n",
    "    partitions = np.concatenate(\n",
    "        [\n",
    "            [np.concatenate(data[j:j+dy*tauy:tauy,i:i+dx*taux:taux]) for i in range(nx-(dx-1)*taux)] \n",
    "            for j in range(ny-(dy-1)*tauy)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    symbols = np.apply_along_axis(np.argsort, 1, partitions)\n",
    "    symbols, symbols_count = np.unique(symbols, return_counts=True, axis=0)\n",
    "\n",
    "    probabilities = symbols_count/len(partitions)\n",
    "\n",
    "    if return_missing==False:\n",
    "        return symbols, probabilities\n",
    "    \n",
    "    else:\n",
    "        all_symbols   = list(map(list,list(itertools.permutations(np.arange(dx*dy)))))\n",
    "        miss_symbols  = setdiff(all_symbols, symbols)\n",
    "        symbols       = np.concatenate((symbols, miss_symbols))\n",
    "        probabilities = np.concatenate((probabilities, np.zeros(miss_symbols.__len__())))\n",
    "        \n",
    "        return symbols, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd32e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_entropy(data, dx=3, dy=1, taux=1, tauy=1, base=2, normalized=True, probs=False, tie_precision=None):\n",
    "    '''\n",
    "    Returns Permutation Entropy\n",
    "    Attributes:\n",
    "    data : array\n",
    "           Array object in the format :math:`[x_{1}, x_{2}, x_{3}, \\\\ldots ,x_{n}]`\n",
    "           or  :math:`[[x_{11}, x_{12}, x_{13}, \\\\ldots, x_{1m}],\n",
    "           \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, \\\\ldots, x_{nm}]]`\n",
    "           or an ordinal probability distribution (such as the ones returned by :func:`ordpy.ordinal_distribution`).\n",
    "    dx :   int\n",
    "           Embedding dimension (horizontal axis) (default: 3).\n",
    "    dy :   int\n",
    "           Embedding dimension (vertical axis); it must be 1 for time series (default: 1).\n",
    "    taux : int\n",
    "           Embedding delay (horizontal axis) (default: 1).\n",
    "    tauy : int\n",
    "           Embedding delay (vertical axis) (default: 1).\n",
    "    base : str, int\n",
    "           Logarithm base in Shannon's entropy. Either 'e' or 2 (default: 2).\n",
    "    normalized: boolean\n",
    "                If `True`, permutation entropy is normalized by its maximum value \n",
    "                (default: `True`). If `False`, it is not.\n",
    "    probs : boolean\n",
    "            If `True`, assumes **data** is an ordinal probability distribution. If \n",
    "            `False`, **data** is expected to be a one- or two-dimensional \n",
    "            array (default: `False`). \n",
    "    tie_precision : int\n",
    "                    If not `None`, **data** is rounded with `tie_precision`\n",
    "                    number of decimals (default: `None`).\n",
    "    '''\n",
    "    if not probs:\n",
    "        _, probabilities = ordinal_distribution(data, dx, dy, taux, tauy, return_missing=False, tie_precision=tie_precision)\n",
    "    else:\n",
    "        probabilities = np.asarray(data)\n",
    "        probabilities = probabilities[probabilities>0]\n",
    "\n",
    "    if normalized==True and base in [2, '2']:        \n",
    "        smax = np.log2(float(np.math.factorial(dx*dy)))\n",
    "        s    = -np.sum(probabilities*np.log2(probabilities))\n",
    "        return s/smax\n",
    "         \n",
    "    elif normalized==True and base=='e':        \n",
    "        smax = np.log(float(np.math.factorial(dx*dy)))\n",
    "        s    = -np.sum(probabilities*np.log(probabilities))\n",
    "        return s/smax\n",
    "    \n",
    "    elif normalized==False and base in [2, '2']:\n",
    "        return -np.sum(probabilities*np.log2(probabilities))\n",
    "    else:\n",
    "        return -np.sum(probabilities*np.log(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a68ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all other predictor variables from vv_list, vh_list, and vv/vh_list\n",
    "def generate_stastical_features(dataframe):\n",
    "    '''\n",
    "    Returns a  list of statistical features such as min,max,range,mean,auto-correlation,permutation entropy for each of the features\n",
    "    Attributes:\n",
    "    dataframe - DataFrame consisting of VV,VH and VV/VH for a time period\n",
    "    '''\n",
    "    features_list = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        min_vv = min(row[0])\n",
    "        max_vv = max(row[0])\n",
    "        range_vv = max_vv - min_vv\n",
    "        mean_vv = np.mean(row[0])\n",
    "        correlation_vv = acf(row[0])[1]\n",
    "        permutation_entropy_vv = permutation_entropy(row[0], dx=6,base=2, normalized=True) \n",
    "    \n",
    "        min_vh = min(row[1])\n",
    "        max_vh = max(row[1])\n",
    "        range_vh = max_vh - min_vh\n",
    "        mean_vh = np.mean(row[1])\n",
    "        correlation_vh = acf(row[1])[1]\n",
    "        permutation_entropy_vh = permutation_entropy(row[1], dx=6, base=2, normalized=True)\n",
    "    \n",
    "        min_vv_by_vh = min(row[2])\n",
    "        max_vv_by_vh = max(row[2])\n",
    "        range_vv_by_vh = max_vv_by_vh - min_vv_by_vh\n",
    "        mean_vv_by_vh = np.mean(row[2])\n",
    "        correlation_vv_by_vh = acf(row[2])[1]\n",
    "        permutation_entropy_vv_by_vh = permutation_entropy(row[2], dx=6, base=2, normalized=True)\n",
    "    \n",
    "        features_list.append([min_vv, max_vv, range_vv, mean_vv, correlation_vv, permutation_entropy_vv,\n",
    "                          min_vh, max_vh, range_vh,  mean_vh, correlation_vh, permutation_entropy_vh,\n",
    "                          min_vv_by_vh,  max_vv_by_vh, range_vv_by_vh, mean_vv_by_vh, correlation_vv_by_vh, permutation_entropy_vv_by_vh])\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112f11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
